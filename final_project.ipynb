{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perspective API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "import json\n",
    "\n",
    "API_KEY = 'AIzaSyBGPPzdZ-xHl3f2CCR9ZDURAF1NMf2U8dY'\n",
    "\n",
    "client = discovery.build(\n",
    "  \"commentanalyzer\",\n",
    "  \"v1alpha1\",\n",
    "  developerKey=API_KEY,\n",
    "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "  static_discovery=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perspective_api(text):\n",
    "  analyze_request = {\n",
    "    'comment': { 'text': text },\n",
    "    'requestedAttributes': {'TOXICITY': {}}\n",
    "  }\n",
    "\n",
    "  response = client.comments().analyze(body=analyze_request).execute()\n",
    "  # print(\"Perspective API\", response[\"attributeScores\"][\"TOXICITY\"][\"summaryScore\"][\"value\"])\n",
    "  return response[\"attributeScores\"][\"TOXICITY\"][\"summaryScore\"][\"value\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxigen Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\suman\\anaconda3\\envs\\ccr\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tomh/toxigen_roberta\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"tomh/toxigen_roberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roberta(text):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model(**tokens)\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "    # print(\"RoBERTa\", probabilities.tolist()[0][0], end=\" \")\n",
    "    return probabilities.tolist()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export OPENAI_API_KEY=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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"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\",\n",
    ")\n",
    "\n",
    "labels_ids = {\n",
    "    'Its good': 1,\n",
    "    'Its ok': 0,\n",
    "    'Its bad': -1\n",
    "}\n",
    "labels = list(labels_ids.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_4(texts, train):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": train,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": texts,\n",
    "            },\n",
    "        ]\n",
    "        ,\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "\n",
    "    return chat_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def keep_only_text(input_string):\n",
    "    # Define a regular expression pattern to match non-alphabetic characters\n",
    "    pattern = re.compile('[^a-zA-Z\\s]')\n",
    "\n",
    "    # Use the sub() function to replace non-alphabetic characters with an empty string\n",
    "    result_string = re.sub(pattern, '', input_string)\n",
    "\n",
    "    return result_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for type, folder in [[\"multilinguality\", \"translated\"], [\"robustness\", \"augmented\"]]:\n",
    "    rename = {\"perturbed\": \"action\"} if type == \"robustness\" else {\"translated\": \"action\"}\n",
    "    toxicity_df = pd.read_csv(f\"./{folder}/{folder}_nlpositionality_toxicity_processed.csv\").rename(columns=rename)\n",
    "    length = len(toxicity_df)\n",
    "    texts = \"\"\n",
    "    train = \"\"\"You are a text annotator. List of texts are given seperated by | and they must be annotated according to toxicity. \n",
    "                The labels are toxico and toxicono for toxic and not toxic respectively. \n",
    "                You will reply the texts and its labels in format [{\"text\": \"\", \"label\": \"\"},{\"text\": \"\", \"label\": \"\"}, ...].\"\"\"\n",
    "    gpt_4_results = []\n",
    "\n",
    "    toxicity_df[\"perspective\"] = [k for k in \"0\"*length]\n",
    "    toxicity_df[\"hateroberta\"] = [k for k in \"0\"*length]\n",
    "\n",
    "    for i in range(length):\n",
    "        orig = toxicity_df.iloc[i, 0]\n",
    "        text = toxicity_df.iloc[i, 1]\n",
    "        lang = toxicity_df.iloc[i, 2]\n",
    "        print(f\"{i}/{length}\")\n",
    "        hateroberta = -1\n",
    "        perspective = -1\n",
    "        try:\n",
    "            try:\n",
    "                hateroberta = roberta(text)\n",
    "                perspective = perspective_api(text)\n",
    "            except:\n",
    "                print(f\"Error at {i}/{length} for {text}, {orig}\")\n",
    "                hateroberta = roberta(text[:200] if lang == \"hi\" else text[:335])\n",
    "                perspective = perspective_api(text)\n",
    "        except:\n",
    "            print(\"Skipping\")\n",
    "            hateroberta = -1\n",
    "            perspective = -1\n",
    "        \n",
    "        texts+=f\"{text} | \"\n",
    "    \n",
    "        if (i+1)%7 == 0 or i == length-1:\n",
    "            gpt4_result = gpt_4(texts)\n",
    "            gpt4_result = gpt4_result.choices[0].message.content\n",
    "            gpt4_result = keep_only_text(gpt4_result)\n",
    "            gpt4_result = gpt4_result.split(\"label\")\n",
    "            l = []\n",
    "            for k in gpt4_result:\n",
    "                if 'Its ' in k:\n",
    "                    k = k[k.index('toxico'):]\n",
    "                    k = -1 if \"toxicono\" in k else 1\n",
    "                    l.append(k)\n",
    "            gpt_4_results += l\n",
    "            texts = \"\"\n",
    "\n",
    "        toxicity_df.loc[i, \"hateroberta\"] = hateroberta\n",
    "        toxicity_df.loc[i, \"perspective\"] = perspective\n",
    "\n",
    "    toxicity_df[\"gpt4\"] = gpt_4_results\n",
    "\n",
    "    toxicity_df.to_csv(f\"./nlpositionality_toxicity_processed_{type}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Acceptability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for type, folder in [[\"multilinguality\", \"translated\"], [\"robustness\", \"augmented\"]]:\n",
    "    rename = {\"perturbed\": \"action\"} if type == \"robustness\" else {\"translated\": \"action\"}\n",
    "    social_df = pd.read_csv(f\"./{folder}/{folder}_nlpositionality_social-acceptability_processed.csv\").rename(columns=rename)\n",
    "    length = len(social_df)\n",
    "    texts = \"\"\n",
    "    train = \"\"\"You are a text annotator. List of texts are given seperated by | and they must be annotated according to toxicity. \n",
    "                The labels are 'Itsnmpgood', 'Itsnmpok', 'Itsnmpbad'. You will reply the texts and its labels in format [{\"text\": \"\", \"label\": \"\"},{\"text\": \"\", \"label\": \"\"}, ...].\"\"\"\n",
    "    \n",
    "    gpt_4_results = []\n",
    "\n",
    "    for i in range(length):\n",
    "        orig = social_df.iloc[i, 0]\n",
    "        text = social_df.iloc[i, 1]\n",
    "        lang = social_df.iloc[i, 2]\n",
    "        print(f\"{i}/{length}\")\n",
    "        \n",
    "        texts+=f\"{text} | \"\n",
    "    \n",
    "        if (i+1)%7 == 0 or i == length-1:\n",
    "            gpt4_result = gpt_4(texts)\n",
    "            gpt4_result = gpt4_result.choices[0].message.content\n",
    "            gpt4_result = keep_only_text(gpt4_result)\n",
    "            gpt4_result = gpt4_result.split(\"label\")\n",
    "            l = []\n",
    "            for k in gpt4_result:\n",
    "                if 'Its ' in k:\n",
    "                    k = k[k.index('Itsnmp'):]\n",
    "                    k = -1 if \"Itsnmpbad\" in k else 0 if \"Itsnmpok\" in k else 1 if \"Itsnmpgood\" in k else 2\n",
    "                    l.append(k)\n",
    "            gpt_4_results += l\n",
    "            texts = \"\"\n",
    "\n",
    "    social_df[\"gpt4\"] = gpt_4_results\n",
    "\n",
    "    social_df.to_csv(f\"./nlpositionality_social-acceptability_processed_{type}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilinguality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type in [\"toxicity\", \"social-acceptability\"]:\n",
    "    \n",
    "    if type == \"toxicity\":\n",
    "        names = ['original', 'action', 'lang', 'perspective','hateroberta', 'gpt4']\n",
    "    else:\n",
    "        names = ['original', 'action', \"lang\", \"gpt4\"]\n",
    "\n",
    "    df1 = pd.read_csv(\"f./nlpositionality_{type}_processed.csv\")\n",
    "    df2 = pd.read_csv(\"f./nlpositionality_{type}_processed_multilinguality.csv\", names=names)\n",
    "    \n",
    "    cols_to_add = list(df1.columns.difference(df2.columns)) + [\"original\", \"lang\"]\n",
    "    results_df = pd.DataFrame([], columns=df1.columns)\n",
    "    length = len(df1)\n",
    "\n",
    "    lisst = {\"original\":[], \"lang\":[]}\n",
    "\n",
    "    for i in df1.columns:\n",
    "        lisst[i] = []\n",
    "\n",
    "    new_columns = ([\"original\", \"lang\"]+df1.columns.to_list())\n",
    "        \n",
    "    for i in range(length):\n",
    "        print(f\"{i+1}/{length}\")\n",
    "        l = df1.iloc[i]\n",
    "        rs = df2[df2[\"original\"] == l[\"action\"]]\n",
    "        for j in cols_to_add[:-2]:\n",
    "            rs[j] = [l[j] for _ in range(7)]\n",
    "        for i in new_columns:\n",
    "            lisst[i]+=(list(rs[i]))\n",
    "\n",
    "    lisst\n",
    "    results_df = pd.DataFrame(lisst)\n",
    "    results_df.to_csv(f\"nlpositionality_{type}_processed_multilinguality.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type in [\"toxicity\", \"social-acceptability\"]:\n",
    "    \n",
    "    if type == \"toxicity\":\n",
    "        names = ['original', 'perturbed', 'perspective', 'hateroberta', 'gpt4']\n",
    "    else:\n",
    "        names = ['original', 'perturbed', 'gpt4']\n",
    "\n",
    "    df1 = pd.read_csv(\"f./nlpositionality_{type}_processed.csv\")\n",
    "    df2 = pd.read_csv(\"f./nlpositionality_{type}_processed_robustness.csv\", names=names)\n",
    "    \n",
    "    cols_to_add = list(df1.columns.difference(df2.columns))\n",
    "    results_df = pd.DataFrame([], columns=df1.columns)\n",
    "    length = len(df1)\n",
    "\n",
    "    lisst = {\"original\":[]}\n",
    "\n",
    "    for i in df1.columns:\n",
    "        lisst[i] = []\n",
    "\n",
    "    new_columns = ([\"original\"]+df1.columns.to_list())\n",
    "        \n",
    "    for i in range(length):\n",
    "        print(f\"{i+1}/{length}\")\n",
    "        l = df1.iloc[i]\n",
    "        rs = df2[df2[\"original\"] == l[\"action\"]]\n",
    "        for j in cols_to_add:\n",
    "            rs[j] = [l[j] for _ in range(7)]\n",
    "        for i in new_columns:\n",
    "            lisst[i]+=(list(rs[i]))\n",
    "\n",
    "    lisst\n",
    "    results_df = pd.DataFrame(lisst)\n",
    "    results_df.to_csv(f\"nlpositionality_{type}_processed_robustness.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # Supress pandas warnings\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "SOCIAL_ACCEPTABILITY = \"social-acceptability\"\n",
    "TOXICITY = \"toxicity\"\n",
    "\n",
    "SOCIAL_CHEM = \"socialchem\"\n",
    "DELPHI = \"delphi\"\n",
    "GPT4 = \"gpt4\"\n",
    "DYNAHATE = \"dynahate\"\n",
    "HATEROBERTA = \"hateroberta\"\n",
    "PERSPECTIVE_API = \"perspective\"\n",
    "REWIRE = \"rewire\"\n",
    "\n",
    "def get_pearson_rs(task, model_or_dataset_name, dataset_type=\"raw\"):\n",
    "    \"\"\"\n",
    "        Inputs:\n",
    "            - task (string): Task name (either \"social acceptability\" or \"toxicity\")\n",
    "            - model_or_dataset_name (string): Name of a model or dataset (either \"socialchem\", \"delphi\", \"gpt4\", \"dynahate\",\n",
    "              \"hateroberta\", \"perspective\", or \"rewire\")\n",
    "        Outputs:\n",
    "            - pearson_rs (DataFrame): DataFrame representing the Pearson's r coefficients and p-values between the dataset\n",
    "              labels/model scores and LabintheWild volunteer annotations by demographic.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Processed dataset is already downloaded via the API\n",
    "    df = pd.read_csv('nlpositionality_{}_{}.csv'.format(task, dataset_type))\n",
    "\n",
    "    results = {}\n",
    "    pvalues = []\n",
    "    for c in ['country_longest', 'education', 'ethnicity', 'gender', 'native_language', 'age', 'country_residence', 'religion']:\n",
    "        demo = list(df[c].unique()) # Get all the demographics under a category\n",
    "        if np.nan in demo:\n",
    "            demo.remove(np.nan)\n",
    "        print(demo)\n",
    "        demo.sort()\n",
    "\n",
    "        if 'None' in demo:\n",
    "            demo.remove('None')\n",
    "\n",
    "        if 'mixed' in demo:\n",
    "            demo.remove('mixed')\n",
    "\n",
    "        if 'arab-american' in demo:\n",
    "            demo.remove('arab-american')\n",
    "\n",
    "        for d in demo:\n",
    "            ndf = df[df[c] == d] # Get all the instances from a demographic group\n",
    "            dndf = __mean_df__(ndf, model_or_dataset_name) # Average responses in a demographic group\n",
    "            r, p = stats.pearsonr(dndf['litw'], dndf[model_or_dataset_name]) # Compute Pearson R values\n",
    "\n",
    "            results[c + '_' + d] = r\n",
    "            pvalues.append(p)\n",
    "\n",
    "    assert(len(results) == len(pvalues))\n",
    "\n",
    "    # Apply Berforroni stepwise correction\n",
    "    alpha = 0.001\n",
    "    hypotheses, pvalues, _, new_alpha = multipletests(pvalues, alpha, method='bonferroni', is_sorted=False, returnsorted=False)\n",
    "\n",
    "    data = []\n",
    "    for key, p, h in zip(results.keys(), pvalues, hypotheses):\n",
    "        # Convert p-values and Pearson's rs to strings\n",
    "        p = str(p)\n",
    "\n",
    "        value = str(round(results[key], 2))\n",
    "        if len(value) == 3:\n",
    "            value += \"0\"\n",
    "        value = value + '' + ('*' if h == True else '')\n",
    "\n",
    "        data.append({\n",
    "            \"demographic\": key,\n",
    "            \"pearson's r\": value,\n",
    "            \"p-value\": p\n",
    "        })\n",
    "\n",
    "    pearson_rs = pd.DataFrame(data=data) # Convert the data to a DataFrame\n",
    "    return pearson_rs\n",
    "\n",
    "def __mean_df__(df, model_or_dataset_name):\n",
    "    ddf = df.groupby(['action']).mean()[['litw', model_or_dataset_name]].reset_index()\n",
    "    ddf['litw'] = ddf['litw'].apply(lambda x: round(x))\n",
    "    return ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\n",
    "    (SOCIAL_ACCEPTABILITY, SOCIAL_CHEM),\n",
    "    (SOCIAL_ACCEPTABILITY, DELPHI),\n",
    "    (SOCIAL_ACCEPTABILITY, GPT4),\n",
    "    (TOXICITY, DYNAHATE),\n",
    "    (TOXICITY, PERSPECTIVE_API),\n",
    "    (TOXICITY, REWIRE),\n",
    "    (TOXICITY, HATEROBERTA),\n",
    "    (TOXICITY, GPT4),\n",
    "]\n",
    "\n",
    "def run_experiments(dataset_type):\n",
    "\n",
    "    for task, dataset_or_model in experiments:\n",
    "        print(task, dataset_or_model)\n",
    "        df = get_pearson_rs(task, dataset_or_model, dataset_type=dataset_type)\n",
    "        print(df)\n",
    "        df.to_csv(f\"./results/{dataset_type.split('_')[-1]}/{task}_{dataset_or_model}_{dataset_type}_output.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_type = \"processed\"\n",
    "    for rob_or_mul in [\"robustness\", \"multilinguality\"]:\n",
    "        run_experiments(f\"{dataset_type}_{rob_or_mul}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
